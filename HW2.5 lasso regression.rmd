---
title: "HW2.5 lasso"
author: "Wuyue Yu"
date: "10/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### The broad steps of Machine learning in R. 

1. Split the data into training and test. Set test aside. 

2. Fit a good model to the training data. This includes using bootstapping, cross validation etc. to resample the training data and fit a good model.

3. Visualize if your model learned on the training data by looking at ROC curve and AUC.

4. Test how your model performs on the test data. 

### Broad steps for choosing between models according to Max Kuhn and Kjell Johnson

1. Start with several models that are the least interpretable and the most flexible, like boosted trees and svms. These models are the often the most accurate.

2. Investigate simpler models that are less opaque, like partial least squares, generalized additive models, or naive bayes models.

3. Consider using the simplest model that reasonable approximates the performance of more complex models

\newpage

```{r, include=FALSE}
library(caret)
library(ROCR)
library(pROC)
library(MASS)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(ggfortify)
library(glmnet)
library(tidyverse)
```

Split data into training and test set
```{r}
train_size <- floor(0.75 * nrow(airquality))
set.seed(543)
train_pos <- sample(seq_len(nrow(airquality)), size = train_size)
train_regression <- airquality[train_pos,-c(1,2)]
test_regression <- airquality[-train_pos,-c(1,2)]

dim(train_regression)
dim(test_regression)
```

## Ridge Regression

$$Ridge Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p(w_j)^2$$
2. Create and train model 
```{r}
ctrl =  trainControl(method = "boot", 15)

Ridge_regression <- train(Temp ~ Wind + Month, data = train_regression,
                          method = 'ridge', trControl= ctrl) 
```

```{r}
Ridge_regression 
```

Examine the residuals 
```{r}
ridge_test_pred <- predict(Ridge_regression, newdata = test_regression)

#plot the predicted values vs the observed values
plot_ridge_test_pred <- data.frame(Temp_test_pred = ridge_test_pred, 
                                   Observed_Temp = test_regression$Temp)
ggplot(data = plot_ridge_test_pred) +
  geom_point(aes(x=Observed_Temp, y = Temp_test_pred)) + 
  ggtitle("True Temp Value vs Predicted Temp Value Ridge Regression") +
  theme_bw()

#median residual value should be close to zero
median(resid(Ridge_regression))
```


# Homework

## Lasso

$$Lasso Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p|w_j|$$
2. Create and train model 
```{r}
ctrl =  trainControl(method = "boot", 20)

Lasso_regression <- train(Temp ~ Wind + Month, data = train_regression,
                          method = 'lasso', trControl= ctrl) 
```

```{r}
Lasso_regression 
```

Examine the residuals 
```{r}
lasso_test_pred <- predict(Lasso_regression, newdata = test_regression)
#plot the predicted values vs the observed values
plot_lasso_test_pred <- data.frame(Temp_test_pred = lasso_test_pred, 
                                   Observed_Temp = test_regression$Temp)
ggplot(data = plot_lasso_test_pred) +
  geom_point(aes(x=Observed_Temp, y = Temp_test_pred)) + 
  ggtitle("True Temp Value vs Predicted Temp Value Lasso Regression") +
  theme_bw()

#median residual value should be close to zero
median(resid(Lasso_regression))
```

